
集成算法：
- 投票选举bagging：在做投票选举的时候可以并行计算，也就是说K个专家在判断的时候是互相独立，不存在依赖性。
- 再学习boosting：boosting的含义是提升，它的作用是每一次训练的时候都对上一次的训练进行改进提升，在训练的过程中这K个专家之间是有依赖性的，当引入第K个专家（第K个分类器）的时候，实际上是对钱K-1个专家的优化。

强分类器可以通过一系列的弱分类器根据不同的权重组合而成。  
根据弱分类器对样本的分类错误率来决定它的权重。

AdaBoost算法是通过改变样本的数据分布来实现的。AdaBoost会判断每次训练的样本是否正确分类，对于正确分类的样本，降低它的权重，对于被错误分类的样本，增加它的权重。再基于上一次得到的分类准确率，来确定这次训练样本中每一个样本的权重。然后将修改过权重的新数据集传递给下一层的分类器进行训练。这样做的好处就是，通过每一轮训练样本的动态权重，可以让训练的焦点集中到难分类的样本上，最终得到的弱分类器的组合更容易得到更高的分类准确率。

第k+1轮训练中，样本的权重集合：  
$$D_{k+1}=(w_{k+1,1},w_{k+1,2},w_{k+1,3},...,w_{k+1,N})$$
其中，$w_{k+1,1}$代表第k+1轮中第一个样本的权重，以此类推$w_{k+1,N}$代表第k+1轮中第N个样本的权重。  
第k+1轮中的第i个样本权重，是根据该样本在第k轮的权重以及第k个分类器的准确率而定，具体的公式是：
$$w_{k+1,i}=\frac{w_{k,i}}{Z_k}exp(-a_ky_iG_k(x_i)),i=1,2,...,N$$
其中，a就是第k个分类器的权重，x是数据，y是分类结果，G是第k个分类器，$Z_k$是归一化因子，就是exp那堆的和，为了使得$w_{k+1,i},i=1,2,...,N$的和是1。

[demo.py](./demo.py)

### 总结
AdaBoost算法是一种集成算法，通过训练不同的弱分类器，将这些弱分类器集成起来形成一个强分类器。在每一轮的训练中都会加入一个新的弱分类器，直到达到足够低的错误率或者达到指定的最大迭代次数为止。实际上每一次迭代都会引入新的弱分类器（这个分类器是每一次迭代中计算出来的，是最新的分类器，不是事先准备好的）  

弱分类器的准确率需比随机猜测的效果好一些，比如随机猜测的准确率是50%的话，那么每个弱分类器的准确率只要大于50%就可用。  

在每一轮训练中，我们都需要从众多“臭皮匠”中选择一个拔尖的，也就是这一轮训练评比中的最优“臭皮匠”，对应的就是错误率最低的分类器。当然每一轮的样本权重都会发生变化，这样做的目标是为了让之前的错误分类的样本得到更多的概率的重复训练机会。

![](AdaBoost（上）：如何使用AdaBoost提.png)