# 爬虫
- selenium  
[xpath解析](./31/README.md)   
[xpath解析2](./37/README.md)  

请求后返回html:  
[xpath解析](./38/README.md)  

请求后返回json：
```python
import requests
import json
html=requests.get(url).text#发起Requests请求，得到文本结果
response=json.loads(html,encoding='utf-8')#JSON解析
for image in response['images']:#按键取值
    pass
```

请求后返回图片：
```python
import requests
pic=requests.get(src,timeout=10)#发起请求，得到图片结果，设置超时10秒
fp=open('./test.jpg','wb')#新建一张空图片
fp.write(pic.content)#写入图片内容
fp.close()#关闭图片
```

# 数据清洗
[数据质量](./11/README.md)   
[数据变化](./13/README.md)   
[数据清洗](./19/README.md) ；[数据清洗2](./23/README.md) 

# 数据可视化
[数据可视化](./15/README.md)   
[词云](./38/README.md) 
```python
'''词云'''
import matplotlib
matplotlib.use('Qt4Agg')
from wordcloud import WordCloud
import matplotlib.pyplot as plt
import jieba

def remove_stop_words(f):# 去掉停用词
  stop_words = ['作词', '作曲', '编曲']
  for stop_word in stop_words:
    f = f.replace(stop_word, '')
  return f

def create_word_cloud(f):# 生成词云
  f = remove_stop_words(f)#调用函数remove_stop_words，去掉停词
  jieba.add_word("毛不易")#增加词语
  cut_text = " ".join(jieba.cut(f, cut_all=False, HMM=True))#分词，并以空格间隔拼接
  wc = WordCloud(
    font_path="./38/wc.ttf",
    collocations=False,#词语不重复
    max_words=100,
    width=2000,
    height=1200,
  )
  wordcloud = wc.generate(cut_text)#生成词云
  wordcloud.to_file("./38/wordcloud.jpg")# 写词云图片
  plt.imshow(wordcloud)
  plt.axis("off")
  plt.show()#调用函数plt_show画图

create_word_cloud(all_word)#调用函数create_word_cloud，生成词云
```
# 分类算法
## ID3
- 采用信息熵作为标准  
[详情](./19/README.md) 
```python
'''ID3分类树'''
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import cross_val_score

clf = DecisionTreeClassifier(criterion='entropy')#ID3决策树分类器，分类标准是信息熵
clf.fit(train_features, train_labels)#放入训练集的特征和分类标识进行训练
pred_labels = clf.predict(test_features)# 决策树预测
# 得到决策树准确率（有测试集的test_labels才用它，否则用K 折交叉验证）
acc_decision_tree = round(clf.score(test_features, test_labels), 6)
print(u'score 准确率为 %.4lf' % acc_decision_tree)
# 使用 K 折交叉验证 统计决策树准确率
print(u'cross_val_score 准确率为 %.4lf' % np.mean(cross_val_score(clf, train_features, train_labels, cv=10)))
```
## C4.5
- 采用信息增益率作为标准  
`sklearn` 自带的决策树模型只能做ID3和CART决策树
## CART
- 采用基尼系数作为标准  
[详情](./18/README.md) 
```python
'''CART分类树'''
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from sklearn.tree import DecisionTreeClassifier

train_test_split(features, labels, test_size=0.33, random_state=0)# 随机抽取 33% 的数据作为测试集，其余为训练集
clf = DecisionTreeClassifier(criterion='gini')#初始化决策树分类器，以基尼系数为分类指标
clf = clf.fit(train_features, train_labels)#放入训练集，包括特征集和分类标识，进行训练
test_predict = clf.predict(test_features)#放入测试集的特征集，进行预测
score = accuracy_score(test_labels, test_predict)#计算准确率
print("CART 分类树准确率 %.4lf" % score)
```
```python
'''CART回归树'''
from sklearn.metrics import mean_squared_error
from sklearn.model_selection import train_test_split
from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error
from sklearn.tree import DecisionTreeRegressor

train_features, test_features, train_price, test_price = train_test_split(features, prices, test_size=0.33, random_state=0)# 随机抽取 33% 的数据作为测试集，其余为训练集

dtr = DecisionTreeRegressor()#初始化 CART 回归树
dtr = dtr.fit(train_features, train_price)#放入训练集，包括特征集和房价，进行训练
predict_price = dtr.predict(test_features)#放入测试集的特征集，进行预测
# 预测结果与测试集结果作比对
print('回归树二乘偏差均值',mean_squared_error(test_price,predict_price))
print('回归树绝对值偏差均值',mean_absolute_error(test_price,predict_price))
```
```python
'''决策树/回归树可视化'''
from sklearn import tree
import graphviz#先在终端跑pip install graphviz
dot_data=tree.export_graphviz(clf,out_file=None)#输出DOT格式的决策树
graph=graphviz.Source(dot_data)
graph.render("分类树","./18/")#会在目录下生成一个pdf
```
## 朴素贝叶斯
朴素贝叶斯：基于概率进行分类（类别概率和条件概率）  
朴素贝叶斯分类常用于文本分类，尤其对于英文等语言来说，分类效果很好。它常用于垃圾文本过滤、情感预测、推荐系统等。 
注：多项式朴素贝叶斯传入的数据不能有负数
[详情](./21/README.md) 
```python
from sklearn.naive_bayes import MultinomialNB
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn import metrics

stop_words=[line.strip() for line in open('./stopword.txt','r',encoding='utf-8-sig').readlines()]
tf = TfidfVectorizer(stop_words=stop_words, max_df=0.5)#stop_words为'english'或list,并过滤掉超过max_df的词语
train_features = tf.fit_transform(train_news.data)#转为文档词条矩阵，作为特征集，注意用的是fit_transform,即训练+转换
clf = MultinomialNB(alpha=0.001).fit(train_features, train_news.target)#多项式贝叶斯分类器
test_features=tf.transform(test_news.data)#要用和训练集一样的词语作为特征集，注意用的是_transform,即仅转换
predicted_labels=clf.predict(test_features)#预测测试集
print('accuracy_score:',metrics.accuracy_score(test_news.target, predicted_labels))#计算准确率
```
## SVM（支持向量机）
SVM是一个寻找超平面（使得类别最大程度的区分开）的过程
[详情](./23/README.md) 
```python
'''svm'''
from sklearn import svm
from sklearn import metrics

model = svm.SVC()# 创建SVM分类器
model.fit(train_X,train_y)# 用训练集做训练
prediction=model.predict(test_X)# 用测试集做预测
print('准确率: ', metrics.accuracy_score(prediction,test_y))#准确率
```
```python
'''linear svm'''
from sklearn import svm
from sklearn import metrics

model = svm.LinearSVC()#linearsvm
model.fit(train_X,train_y)# 用训练集做训练
prediction=model.predict(test_X)# 用测试集做预测
print('准确率: ', metrics.accuracy_score(prediction,test_y))#准确率
```
## KNN
对于待分类物而言，它的K个最近的邻居中哪个类最多，那它就属于谁
[详情](./25/README.md) 
```python
from sklearn.metrics import accuracy_score
from sklearn.neighbors import KNeighborsClassifier

knn = KNeighborsClassifier()# 创建KNN分类器
knn.fit(train_ss_x, train_y) #训练
predict_y = knn.predict(test_ss_x) #预测
print("KNN准确率: %.4lf" % accuracy_score(test_y,predict_y))#KNN准确率: 0.9756
```
## 逻辑回归
逻辑回归适用于分类结果严重不平衡的二分类问题
[详情](./40/README.md) 
```python
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import confusion_matrix, precision_recall_curve

clf = LogisticRegression()# 逻辑回归分类
clf.fit(train_x, train_y)#训练
predict_y = clf.predict(test_x)#预测（0或1）
score_y = clf.decision_function(test_x)# 预测样本的置信分数#0-1的数值
cm = confusion_matrix(test_y, predict_y)#混淆矩阵

# 显示模型评估结果
def show_metrics(cm):
  tp = cm[1, 1]
  fn = cm[1, 0]
  fp = cm[0, 1]
  tn = cm[0, 0]
  P=tp / (tp + fp)
  R=tp / (tp + fn)
  F1=2*P*R/(P+R)
  print('精确率: {:.3f}'.format(P))
  print('召回率: {:.3f}'.format(R))
  print('F1值: {:.3f}'.format(F1))
show_metrics(cm)#调用show_metrics函数，显示模型评估分数
```
## 深度学习
深度学习属于机器学习的一种，它的目标同样是让机器具有智能，但它是通过神经网络来实现的。
[详情](./43/README.md) 
```python
import keras
from keras.layers import Conv2D, MaxPooling2D
from keras.layers import Dense, Flatten
from keras.models import Sequential

# 【1】创建序贯模型
model = Sequential()
# 【2】第一层卷积层：6 个卷积核，大小为 5∗5, relu 激活函数
model.add(Conv2D(6, kernel_size=(5, 5), activation='relu', input_shape=(28, 28, 1)))
# 【3】第二层池化层：最大池化
model.add(MaxPooling2D(pool_size=(2, 2)))
# 【4】第三层卷积层：16 个卷积核，大小为 5*5，relu 激活函数
model.add(Conv2D(16, kernel_size=(5, 5), activation='relu'))
# 【5】第二层池化层：最大池化
model.add(MaxPooling2D(pool_size=(2, 2)))
# 【6】将参数进行扁平化，在 LeNet5 中称之为卷积层，实际上这一层是一维向量，和全连接层一样
model.add(Flatten())
model.add(Dense(120, activation='relu'))
# 【7】全连接层，输出节点个数为 84 个
model.add(Dense(84, activation='relu'))
# 【8】输出层 用 softmax 激活函数计算分类概率
model.add(Dense(10, activation='softmax'))
# 【9】设置损失函数和优化器配置
model.compile(loss=keras.metrics.categorical_crossentropy, optimizer=keras.optimizers.Adam(), metrics=['accuracy'])

# 传入训练数据进行训练，epochs控制训练次数
model.fit(train_x, train_y, batch_size=128, epochs=2, verbose=1, validation_data=(test_x, test_y))
score = model.evaluate(test_x, test_y)# 对结果进行评估
print('误差:%0.4lf' % score[0])
print('准确率:', score[1])
```
## Adaboost
Adaboost属于分类算法中的集成算法，它训练多个弱分类器，将它们组合成一个强分类器
[详情](./35/README.md) 
```python
'''AdaBoost回归模型'''
from sklearn.metrics import mean_squared_error
from sklearn.ensemble import AdaBoostRegressor

regressor=AdaBoostRegressor()# 使用 AdaBoost 回归模型
regressor.fit(train_x,train_y)#训练
pred_y = regressor.predict(test_x)#预测
mse = mean_squared_error(test_y, pred_y)#均方误差
```
```python
'''AdaBoost 分类器'''
from sklearn.metrics import zero_one_loss
from sklearn.ensemble import AdaBoostClassifier

ada = AdaBoostClassifier(random_state=1,n_estimators=10)#对弱分类器进行迭代
ada.fit(train_x, train_y)
predict_y = ada.predict(test_x)#预测
print("准确率 %0.4lf" % accuracy_score(test_y, predict_y))#准确率
```
## 随机森林
随机森林实际上是一个包含多个决策树的分类器，属于集成算法
[详情](./39/README.md)
```python
# 利用 GridSearchCV 寻找最优参数, 使用 Pipeline 进行流水作业
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import GridSearchCV
from sklearn.pipeline import Pipeline
from sklearn.metrics import accuracy_score

rf = RandomForestClassifier()#随机森林
parameters = {"randomforestclassifier__n_estimators": range(1,11)}#决策树个数范围是1-10
pipeline = Pipeline([
        ('scaler', StandardScaler()),#先标准化
        ('randomforestclassifier', rf)#再做随机森林
])
# 使用 GridSearchCV 进行参数调优
clf = GridSearchCV(estimator=pipeline, param_grid=parameters, scoring='accuracy')
clf.fit(train_x, train_y)#训练数据
print(" 最优分数： %.4lf" %clf.best_score_)#按评价标准最好的
print(" 最优参数：", clf.best_params_)
predict_y = clf.predict(test_x)#预测
print("准确率 %0.4lf" % accuracy_score(test_y, predict_y))#准确率
```
# 聚类算法
## K-Means
[详情](./26/README.md)
```python
from sklearn.cluster import KMeans

kmeans = KMeans(n_clusters=3)#kmeans聚类，3个聚类中心
kmeans.fit(train_x)#进行聚类
predict_y = kmeans.predict(train_x)#聚类结果
result = pd.concat((data, pd.DataFrame(predict_y)), axis=1)# 合并聚类结果，插入到原数据中
result.rename({0: u'聚类'}, axis=1, inplace=True)#将0这个字段名修改为“聚类”
```
## EM（最大期望值算法）
EM聚类将聚类问题转化为参数估计问题，把潜在类别当做隐藏变量，样本看做观察值
[详情](./29/README.md)
```python
from sklearn.mixture import GaussianMixture
from sklearn.metrics import calinski_harabaz_score

gmm = GaussianMixture(n_components=30, covariance_type='full')#高斯混合模型，30个类(n_components是高斯混合模型的个数，也就是聚类个数）
gmm.fit(data)#训练数据
prediction = gmm.predict(data)# 预测
data_ori.insert(0, '分组', prediction)#插入一列数据
data_ori.to_csv('./29/hero_out.csv', index=False, sep=',')#导出数据到csv文件
print(calinski_harabaz_score(data,prediction))# 评估聚类结果
```
# 关联分析
## Apriori
Apriori其实是查找频繁项集的过程（从数据集中发现项与项之间的关系）
[详情](./31/README.md)
```python
from efficient_apriori import apriori
# 设置数据集（一个list数组类型，里面的值可以是集合也可以是列表）
data = [['牛奶','面包','尿布'],
           ['可乐','面包', '尿布', '啤酒'],
           ['牛奶','尿布', '啤酒', '鸡蛋'],
           ['面包', '牛奶', '尿布', '啤酒'],
           ['面包', '牛奶', '尿布', '可乐']]
itemsets, rules = apriori(data, min_support=0.5,  min_confidence=1)# 挖掘频繁项集和频繁规则
print(itemsets)
print(rules)
```
# 连接分析
## PageRank
[详情](./33/README.md)
```python
import networkx as nx
from collections import defaultdict
import matplotlib.pyplot as plt

def show_graph(graph):# 画网络图
    positions=nx.spring_layout(graph)# 使用Spring Layout布局，类似中心放射状
    # 设置网络图中的节点大小，大小与pagerank值相关，因为pagerank值很小所以需要*20000
    nodesize = [x['pagerank']*20000 for v,x in graph.nodes(data=True)]#(v,x)如('Jake Sullivan', {'pagerank': 0.005791704166582624})
    # 设置网络图中的边长度
    edgesize = [np.sqrt(e[2]['weight']) for e in graph.edges(data=True)]#e如('Jake Sullivan', 'Hillary Clinton', {'weight': 815})
    nx.draw_networkx_nodes(graph, positions, node_size=nodesize, alpha=0.4)# 绘制节点
    nx.draw_networkx_edges(graph, positions, edge_size=edgesize, alpha=0.2)# 绘制边
    nx.draw_networkx_labels(graph, positions, font_size=10)# 绘制节点的label
    plt.show()# 输出网络图

graph = nx.DiGraph()# 创建一个有向图
graph.add_weighted_edges_from(edges_weights)# 设置有向图中的路径及权重(from, to, weight)
pagerank = nx.pagerank(graph)# 计算每个节点（人）的PR值，并作为节点的pagerank属性
pagerank_list = {node: rank for node, rank in pagerank.items()}# 获取每个节点的pagerank数值
nx.set_node_attributes(graph, name = 'pagerank', values=pagerank_list)# 将pagerank数值作为节点的属性
show_graph(graph)# 画网络图
```
# 时间序列预测
时间序列分析模型建立了观察结果与时间变化的关系   
[详情](./41/README.md)   
[比特币走势预测](./41/bitcoin_analysis.py) 
```python
# 用 ARMA 进行时间序列预测
import pandas as pd
import matplotlib.pyplot as plt
import statsmodels.api as sm
from statsmodels.tsa.arima_model import ARMA
from statsmodels.graphics.api import qqplot

# 创建数据
data = [5922, 5308, 5546, 5975, 2704, 1767, 4111, 5542, 4726, 5866, 6183, 3199, 1471, 1325, 6618, 6644, 5337, 7064, 2912, 1456, 4705, 4579, 4990, 4331, 4481, 1813, 1258, 4383, 5451, 5169, 5362, 6259, 3743, 2268, 5397, 5821, 6115, 6631, 6474, 4134, 2728, 5753, 7130, 7860, 6991, 7499, 5301, 2808, 6755, 6658, 7644, 6472, 8680, 6366, 5252, 8223, 8181, 10548, 11823, 14640, 9873, 6613, 14415, 13204, 14982, 9690, 10693, 8276, 4519, 7865, 8137, 10022, 7646, 8749, 5246, 4736, 9705, 7501, 9587, 10078, 9732, 6986, 4385, 8451, 9815, 10894, 10287, 9666, 6072, 5418]
data=pd.Series(data)#转为时间序列数据
data_index = sm.tsa.datetools.dates_from_range('1901','1990')#1901到1990
data.index = pd.Index(data_index)#90个数据的时间标签是1901到1990

# 绘制数据图
data.plot(figsize=(12,8))
plt.show()

# 创建 ARMA 模型 
arma = ARMA(data,(7,0)).fit()#AR的阶数为7，MA的阶数为0
print('AIC: %0.4lf' %arma.aic)#AIC: 1619.6323

# 模型预测
predict_y = arma.predict('1990', '2000')#预测未来十年
# 预测结果绘制
fig, ax = plt.subplots(figsize=(12, 8))#貌似是在原图的基础上添加
ax = data.ix['1901':].plot(ax=ax)#貌似是用来延迟x轴
predict_y.plot(ax=ax)#将预测结果画到延长的x轴上
plt.show()
```