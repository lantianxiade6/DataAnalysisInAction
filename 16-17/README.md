## 16是答疑课而已

## 决策树1

1. 构造：选择什么属性作为节点的过程   
选择哪个属性作为根节点   
选择哪些属性作为子节点   
什么时候停止并得到目标状态，及叶节点  
**根据指标：选择指标最好的那个属性作为根节点，子节点如此类推。**
2. 剪枝：
为防止过拟合，泛化能力差  
泛化能力：分类器通过训练集抽象出来的分类能力  

预剪枝：在构造的过程中对节点进行评估，如果对某个节点进行划分，在验证集中不能带来准确性的提升，那么对这个节点进行划分就没有意义，这时就会把当前节点作为叶节点，不再对其进行划分。   

后剪枝：在生成决策树之后再进行剪枝，通常会从决策树的叶节点开始，逐层向上对每个节点就行评估。如果减掉这个节点子树，与保留该节点子树在分类准确性上的差别不大，或者能在验证集中带来准确性的提升，那么就可以把该节点子树的叶子节点来代替该节点，类标记为这个节点子树中最频繁的那个类。   

3. 指标  
纯度（分歧最小）  
信息熵（信息的不确定度），信息熵越大，纯度越低  
不纯度：信息增益、信息增益率、基尼系数

4. 经典算法   
**ID3算法**  
指标：信息增益=父节点的信息熵-所有子节点的信息熵 
优点：规则简单，可解释性强   
缺点：对噪声敏感，倾向于选择属性值多的属性，某些属性作用不大却被选为最优属性  
**C4.5算法**  
指标：信息增益率=信息增益/属性熵（解决了对噪声敏感的问题）  
采用悲观剪枝：（后剪枝的一种）比较剪枝前后这个节点的分类错误率来决定是否对其进行剪枝  
离散化处理连续属性：选择具有最高信息增益的划分来离散化连续属性  
处理缺失值
缺点：需要对数据进行多次扫描，算法效率相对较低


## 作业 
编号|红|大|好苹果
---|:--:|:--:|---:
1|是|是|是
2|是|否|是
3|否|是|否
4|否|否|否

根节点信息熵=-2/4log2(2/4)-2/4log2(2/4)=1  
“红”属性划分的信息熵=1/2x红为是的信息熵+1/2x红为否的信息熵=1/2x(0)——1/2x(0)=0  
“大”是属性划分的信息熵=1/2x大为是的信息熵+1/2x大为否的信息熵=1/2x{-1/2log2(1/2)-1/2log2(1/2)}+1/2x{-1/2log2(1/2)-1/2log2(1/2)}=1/2x1+1/2x1=1  
gain(D,红)=根节点信息熵-“红”属性划分的信息熵=1-0=1  
gain(D,大)=根节点信息熵-“大”属性划分的信息熵=1-1=0  
所以选“红”作为跟节点，而后由于“大”做不做属性划分都对树没有影响，所以直接剪枝。 

结果就是：
红=是->好苹果；红=否->不是好苹果。 


![决策树1](./决策树1.png)